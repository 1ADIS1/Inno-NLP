{"cells":[{"cell_type":"markdown","metadata":{"id":"DIgM6C9HYUhm"},"source":["# Context-sensitive Spelling Correction\n","\n","The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n","\n","Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n","\n","Useful links:\n","- [Norvig's solution](https://norvig.com/spell-correct.html)\n","- [Norvig's dataset](https://norvig.com/big.txt)\n","- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n","\n","Grading:\n","- 60 points - Implement spelling correction\n","- 20 points - Justify your decisions\n","- 20 points - Evaluate on a test set\n"]},{"cell_type":"markdown","metadata":{"id":"x-vb8yFOGRDF"},"source":["## Implement context-sensitive spelling correction\n","\n","Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n","\n","The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n","\n","You may also want to implement:\n","- spell-checking for a concrete language - Russian, Tatar, etc. - any one you know, such that the solution accounts for language specifics,\n","- some recent (or not very recent) paper on this topic,\n","- solution which takes into account keyboard layout and associated misspellings,\n","- efficiency improvement to make the solution faster,\n","- any other idea of yours to improve the Norvig’s solution.\n","\n","IMPORTANT:  \n","Your project should not be a mere code copy-paste from somewhere. You must provide:\n","- Your implementation\n","- Analysis of why the implemented approach is suggested\n","- Improvements of the original approach that you have chosen to implement"]},{"cell_type":"code","execution_count":63,"metadata":{"execution":{"iopub.execute_input":"2024-03-22T19:01:23.509567Z","iopub.status.busy":"2024-03-22T19:01:23.508760Z","iopub.status.idle":"2024-03-22T19:01:24.434563Z","shell.execute_reply":"2024-03-22T19:01:24.433322Z","shell.execute_reply.started":"2024-03-22T19:01:23.509532Z"},"trusted":true},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>275</td>\n","      <td>a</td>\n","      <td>a</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>31</td>\n","      <td>a</td>\n","      <td>aaa</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>29</td>\n","      <td>a</td>\n","      <td>all</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>45</td>\n","      <td>a</td>\n","      <td>an</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>192</td>\n","      <td>a</td>\n","      <td>and</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1020380</th>\n","      <td>24</td>\n","      <td>zviad</td>\n","      <td>gamsakhurdia</td>\n","    </tr>\n","    <tr>\n","      <th>1020381</th>\n","      <td>25</td>\n","      <td>zweimal</td>\n","      <td>leben</td>\n","    </tr>\n","    <tr>\n","      <th>1020382</th>\n","      <td>24</td>\n","      <td>zwick</td>\n","      <td>and</td>\n","    </tr>\n","    <tr>\n","      <th>1020383</th>\n","      <td>24</td>\n","      <td>zydeco</td>\n","      <td>music</td>\n","    </tr>\n","    <tr>\n","      <th>1020384</th>\n","      <td>72</td>\n","      <td>zz</td>\n","      <td>top</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1020385 rows × 3 columns</p>\n","</div>"],"text/plain":["           0        1             2\n","0        275        a             a\n","1         31        a           aaa\n","2         29        a           all\n","3         45        a            an\n","4        192        a           and\n","...      ...      ...           ...\n","1020380   24    zviad  gamsakhurdia\n","1020381   25  zweimal         leben\n","1020382   24    zwick           and\n","1020383   24   zydeco         music\n","1020384   72       zz           top\n","\n","[1020385 rows x 3 columns]"]},"execution_count":63,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","\n","\n","bigrams_df = pd.read_csv(\"/kaggle/input/nlp-assignment2-data/bigrams.txt\", sep='\t', encoding='latin-1', header=None)\n","\n","# Some words somehow convert to floats, so cast them to string first\n","bigrams_df[1] = bigrams_df[1].str.lower()\n","bigrams_df[2] = bigrams_df[2].str.lower()\n","\n","bigrams_df"]},{"cell_type":"code","execution_count":64,"metadata":{"execution":{"iopub.execute_input":"2024-03-22T19:01:24.436997Z","iopub.status.busy":"2024-03-22T19:01:24.436640Z","iopub.status.idle":"2024-03-22T19:01:36.502400Z","shell.execute_reply":"2024-03-22T19:01:36.500499Z","shell.execute_reply.started":"2024-03-22T19:01:24.436968Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: editdistance in /opt/conda/lib/python3.10/site-packages (0.8.1)\n"]}],"source":["!pip install editdistance"]},{"cell_type":"markdown","metadata":{},"source":["### Reference on edit-distances: https://norvig.com/spell-correct.html"]},{"cell_type":"code","execution_count":65,"metadata":{"execution":{"iopub.execute_input":"2024-03-22T19:01:36.504823Z","iopub.status.busy":"2024-03-22T19:01:36.504400Z","iopub.status.idle":"2024-03-22T19:01:36.515609Z","shell.execute_reply":"2024-03-22T19:01:36.514316Z","shell.execute_reply.started":"2024-03-22T19:01:36.504788Z"},"trusted":true},"outputs":[],"source":["def edits_n(word, n=2) -> set:\n","    \"\"\"\n","    Return set of words that are N edit distance away from the given word. \n","    \"\"\"\n","\n","    e = edits1(word)\n","    prev = e\n","    for i in range(1, n):\n","        temp = set(e2 for e1 in prev for e2 in edits1(e1))\n","        e.update(temp)\n","        prev = temp\n","    e.discard(word)\n","    return e\n","\n","\n","def edits1(word):\n","    \"\"\"\n","    All edits that are one edit away from 'word'\n","    \"\"\"\n","    letters = 'abcdefghijklmnopqrstuvwxyz'\n","    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n","\n","    deletes = [L + R[1:] for L, R in splits if R ]\n","    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n","    replaces = [L + c + R[1:] for L, R in splits if R for c in letters ]\n","    inserts = [L + c + R for L, R in splits for c in letters]\n","\n","    return set(deletes + transposes + replaces + inserts)"]},{"cell_type":"code","execution_count":66,"metadata":{"execution":{"iopub.execute_input":"2024-03-22T19:01:36.519730Z","iopub.status.busy":"2024-03-22T19:01:36.519291Z","iopub.status.idle":"2024-03-22T19:01:36.533262Z","shell.execute_reply":"2024-03-22T19:01:36.532136Z","shell.execute_reply.started":"2024-03-22T19:01:36.519698Z"},"trusted":true},"outputs":[],"source":["import random\n","\n","\n","class SentenceGenerator:\n","    \"\"\"\n","    This class generates error sentences.\n","    \n","    Why you cannot make them yourself? \n","    SpellingChecker assumes the error word is within 2 edit distances from the original one,\n","    so it better to have a separate class for satisfying our assumption.\n","    \"\"\"\n","    \n","    def __init__(self, train_data: dict):\n","        self.train_data = train_data\n","    \n","    \n","    def generate_error_word(self, word, edit_distance=2):\n","        \"\"\"\n","        Function for generating test dataset.\n","        Generate error word within edit distance of the original word\n","        \"\"\"\n","        \n","        possible_errors = edits_n(word, edit_distance)\n","        \n","        non_word_errors = []\n","        real_word_errors = []\n","        for error in possible_errors:\n","            if error in self.train_data:\n","                real_word_errors.append(error)\n","            else:\n","                non_word_errors.append(error)\n","        \n","        # With 50% probability choose either \n","        # non-word or real-word error\n","        c = random.randint(0,1)\n","        if c or len(real_word_errors)==0:\n","            return random.choice(non_word_errors)\n","        \n","        return random.choice(real_word_errors)\n","    \n","    def generate_error_sentences(self, sentences: [str], edit_distance = 2, errors_per_sentence = 1) -> [[str]]:\n","        \"\"\"\n","        Input is the correct sentence without errors.\n","        \n","        Assumes that the input is the set of sentences (strings).\n","        Sentences should not include the punctuation.\n","        \n","        Returns sentences with different errors (which might be non-sense too).\n","        \"\"\"\n","        \n","        output = []\n","        for sentence in sentences:\n","            tokenized_data = sentence.lower().split(\" \")\n","                        \n","            # Randomly shuffle words in the sentence and iterate over them.\n","            indices = random.choices(range(len(tokenized_data)), k = errors_per_sentence)\n","            \n","            for i in indices:\n","                tokenized_data[i] = self.generate_error_word(tokenized_data[i], edit_distance)\n","            \n","            output.append(tokenized_data)\n","                        \n","        return output"]},{"cell_type":"code","execution_count":67,"metadata":{"execution":{"iopub.execute_input":"2024-03-22T19:01:36.537643Z","iopub.status.busy":"2024-03-22T19:01:36.537265Z","iopub.status.idle":"2024-03-22T19:01:36.549876Z","shell.execute_reply":"2024-03-22T19:01:36.548726Z","shell.execute_reply.started":"2024-03-22T19:01:36.537614Z"},"trusted":true},"outputs":[],"source":["from math import log10, inf\n","\n","\n","class Ngram:\n","    def __init__(self, grams=2):\n","        self.n = grams\n","        self.train_data = {}\n","        self.max_count = 0\n","    \n","    def train(self, train_df: pd.DataFrame):\n","        for index, row in train_df.iterrows():\n","            count = row[0]\n","            word1 = row[1]\n","            word2 = row[2]\n","\n","            if word1 not in self.train_data:\n","                self.train_data[word1] = []\n","\n","            self.train_data[word1].append((word2, count))\n","            self.max_count += count\n","    \n","    def get_probability(self, context: str, word: str) -> float: \n","        \"\"\"\n","        Returns probability of `word` in the given context.\n","        Context is the string consisting of single word.\n","        \"\"\"            \n","        \n","        count_pair = self.is_word_in_train_data_context(context, word)\n","        \n","        if count_pair:\n","            return log10(count_pair[1] / self.max_count)\n","        \n","        # If there is no such context or word in the dictionary, then we assign minimum probability\n","        return -999\n","    \n","    def is_word_in_train_data_context(self, context: str, word: str):\n","        \"\"\"\n","        Returns pair of (word, count) if the given context (key in dict)\n","        contains the given word.\n","        \"\"\"\n","        \n","        # If there is no such context\n","        if context not in self.train_data:\n","            return None\n","        \n","        for count_pair in self.train_data[context]:\n","            if count_pair[0] != word:\n","                continue\n","                \n","            return count_pair\n","        \n","        return None"]},{"cell_type":"code","execution_count":68,"metadata":{"execution":{"iopub.execute_input":"2024-03-22T19:01:36.552571Z","iopub.status.busy":"2024-03-22T19:01:36.552214Z","iopub.status.idle":"2024-03-22T19:01:36.591286Z","shell.execute_reply":"2024-03-22T19:01:36.589941Z","shell.execute_reply.started":"2024-03-22T19:01:36.552544Z"},"trusted":true},"outputs":[],"source":["import editdistance\n","\n","\n","class SpellingChecker(Ngram):\n","    \"\"\"\n","    Class that implements 2-gram spell checker, given the training data consisting of 3 columns:\n","    (count, word1, word2).\n","    \"\"\"\n","    \n","    def __init__(self, grams=2):\n","        \"\"\"\n","        Initialises train data, which is a dictionary, where each key is a word from column 1 (word1),\n","        and value is a list of tuples (word2, count).\n","        \"\"\"\n","        \n","        super().__init__(grams)\n","\n","    def get_corrected_word(self, context, word, next_word=\"\") -> str:\n","        \"\"\"\n","        Find the most-probable correction for a word, combinining:\n","        (context + word) and (word + next word) as max probability.\n","        \"\"\"\n","        \n","        corrected_word = word\n","        max_prob = self.get_probability(context, word) + self.get_probability(word, next_word)\n","                \n","        # Calculate probability for every word in dictionary\n","        # if it is 2 edit-distances away.\n","        for candidate_word in self.train_data:\n","            context_prob = self.get_probability(context, candidate_word)\n","            word_prob = self.get_probability(candidate_word, next_word)\n","\n","            prob_sum = context_prob + word_prob\n","\n","            if prob_sum > max_prob:\n","                max_prob = prob_sum\n","                corrected_word = candidate_word\n","                                    \n","        return corrected_word\n","    \n","    def find_and_correct_errors(self, data_with_errors: [[str]], max_to_correct = 1) -> [[str]]:\n","        \"\"\"\n","        Detect and correct all errors in a list of sentences.\n","        \"\"\"\n","        \n","        corrected_data = data_with_errors\n","        \n","        # Iterate through sentences\n","        for j in range(len(corrected_data)):\n","            context = ''\n","            words_corrected = 0\n","            \n","            # We will use these 2 values for\n","            # identifying the least probable word, and correct it, in case all\n","            # words are in the dictionary.\n","            min_probability = 999999\n","            min_word_index = None\n","            \n","            # Iterate through words\n","            for i in range(len(corrected_data[j]) - 1):                \n","                if words_corrected == max_to_correct:\n","                    break\n","                \n","                next_word_after_error = corrected_data[j][i + 1]\n","                \n","                context_prob = self.get_probability(context, corrected_data[j][i])\n","                \n","                # Assumption: if the word is not in the train_data,\n","                # it is most likely an error word.\n","                if(corrected_data[j][i] not in self.train_data):\n","                                        \n","                    word_before_correction = corrected_data[j][i]\n","                    corrected_data[j][i] = self.get_corrected_word(context, corrected_data[j][i], next_word_after_error)\n","                                        \n","                    if word_before_correction != corrected_data[j][i]:\n","                        words_corrected += 1\n","                    \n","                context = corrected_data[j][i]\n","                \n","                next_prob = self.get_probability(context, next_word_after_error)\n","                \n","                prob_sum = context_prob + next_prob\n","                if prob_sum < min_probability:\n","                    min_probability = prob_sum\n","                    min_word_index = i\n","                \n","            # If all words are in the train data, then\n","            # find and fix the least probable word\n","            if min_word_index and words_corrected == 0:\n","                \n","                # Some checks for accessing the context and next word\n","                context = ''\n","                if min_word_index > 1:\n","                    context = corrected_data[j][min_word_index - 1]\n","                \n","                next_word_after_error = ''\n","                if min_word_index + 1 < len(corrected_data[j]):\n","                    next_word_after_error = corrected_data[j][min_word_index + 1]\n","                \n","                before = corrected_data[j][min_word_index]\n","                corrected_data[j][min_word_index] = self.get_corrected_word(context, corrected_data[j][min_word_index], next_word_after_error)\n","                        \n","        return corrected_data"]},{"cell_type":"markdown","metadata":{"id":"oML-5sJwGRLE"},"source":["## Justify your decisions\n","\n","Write down justificaitons for your implementation choices. For example, these choices could be:\n","- Which ngram dataset to use\n","- Which weights to assign for edit1, edit2 or absent words probabilities\n","- Beam search parameters\n","- etc."]},{"cell_type":"markdown","metadata":{"id":"6Xb_twOmVsC6"},"source":["### At first, I have used bigrams dataset provided in the assignment description, as larger grams tend to lead to slower train time and requires much larger sets of data to be accurate.\n","\n","### Secondly, I distinguish the words that are present in the train data, and not present, which partially mitigates the problem with large set of data. However, despite that, my solution still depends on large corpus.\n","\n","### Lastly, I am mainly use bigram model to correct the spelling as it is the fastest method, but maybe not the most accurate one. Still, it checks the context of the previous and next words from error word, which it already better than the Norvig's solution."]},{"cell_type":"markdown","metadata":{"id":"46rk65S4GRSe"},"source":["## Evaluate on a test set\n","\n","Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity. Compare your solution to the Norvig's corrector, and report the accuracies."]},{"cell_type":"code","execution_count":69,"metadata":{"execution":{"iopub.execute_input":"2024-03-22T19:01:36.592895Z","iopub.status.busy":"2024-03-22T19:01:36.592576Z","iopub.status.idle":"2024-03-22T19:02:41.611576Z","shell.execute_reply":"2024-03-22T19:02:41.610303Z","shell.execute_reply.started":"2024-03-22T19:01:36.592862Z"},"trusted":true},"outputs":[],"source":["bigram_model = SpellingChecker()\n","bigram_model.train(bigrams_df)"]},{"cell_type":"markdown","metadata":{},"source":["### Probibilities test"]},{"cell_type":"code","execution_count":70,"metadata":{"execution":{"iopub.execute_input":"2024-03-22T19:02:41.617128Z","iopub.status.busy":"2024-03-22T19:02:41.613536Z","iopub.status.idle":"2024-03-22T19:02:41.626532Z","shell.execute_reply":"2024-03-22T19:02:41.624952Z","shell.execute_reply.started":"2024-03-22T19:02:41.617066Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["-999\n","-6.018183160884371\n","-6.966154160880362\n"]}],"source":["print(bigram_model.get_probability(\"a\", \"\"))\n","print(bigram_model.get_probability(\"a\", \"a\"))\n","print(bigram_model.get_probability(\"a\", \"aaa\"))\n","\n","assert bigram_model.get_probability(\"a\", \"a\") > bigram_model.get_probability(\"a\", \"aaa\")"]},{"cell_type":"markdown","metadata":{},"source":["### Small test for 'I' correction"]},{"cell_type":"code","execution_count":71,"metadata":{"execution":{"iopub.execute_input":"2024-03-22T19:02:41.628222Z","iopub.status.busy":"2024-03-22T19:02:41.627892Z","iopub.status.idle":"2024-03-22T19:02:41.768054Z","shell.execute_reply":"2024-03-22T19:02:41.766831Z","shell.execute_reply.started":"2024-03-22T19:02:41.628195Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[['i', 'am', 'a', 'responsible', 'person']]"]},"execution_count":71,"metadata":{},"output_type":"execute_result"}],"source":["error_data = [['eig', 'am', 'a', 'responsible', 'person']]\n","bigram_model.find_and_correct_errors(error_data)"]},{"cell_type":"code","execution_count":72,"metadata":{"execution":{"iopub.execute_input":"2024-03-22T19:02:41.771927Z","iopub.status.busy":"2024-03-22T19:02:41.771424Z","iopub.status.idle":"2024-03-22T19:02:42.026816Z","shell.execute_reply":"2024-03-22T19:02:42.025556Z","shell.execute_reply.started":"2024-03-22T19:02:41.771885Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[['hr', 'am', 'a', 'responsible', 'person'],\n"," ['hi', 'my', 'name', 'qiss', 'john'],\n"," ['hey', 'buddy', 'what', 'are', 'ion', 'doing', 'here'],\n"," ['nlp', 'copse', 'is', 'a', 'good', 'one']]"]},"execution_count":72,"metadata":{},"output_type":"execute_result"}],"source":["sentence_generator = SentenceGenerator(bigram_model.train_data)\n","input_sentences = [\"I am a responsible person\", \"Hi my name is John\", \"Hey buddy what are you doing here\", \"NLP course is a good one\"]\n","error_sentences = sentence_generator.generate_error_sentences(input_sentences)\n","error_sentences"]},{"cell_type":"code","execution_count":73,"metadata":{"execution":{"iopub.execute_input":"2024-03-22T19:02:42.028993Z","iopub.status.busy":"2024-03-22T19:02:42.028506Z","iopub.status.idle":"2024-03-22T19:02:55.466557Z","shell.execute_reply":"2024-03-22T19:02:55.465462Z","shell.execute_reply.started":"2024-03-22T19:02:42.028951Z"},"trusted":true},"outputs":[{"data":{"text/plain":["[['hr', 'am', 'a', 'responsible', 'person'],\n"," ['hi', 'my', 'name', 'of', 'john'],\n"," ['hey', 'buddy', 'what', 'are', 'you', 'doing', 'here'],\n"," ['a', 'copse', 'is', 'a', 'good', 'one']]"]},"execution_count":73,"metadata":{},"output_type":"execute_result"}],"source":["corrected_data = bigram_model.find_and_correct_errors(error_sentences)\n","corrected_data"]},{"cell_type":"markdown","metadata":{},"source":["### In the Norvig's solution there is a abscence of the context checking, which my solution successfully provides with the use of bigrams.\n","\n","### Next, Nordwig provides only 2 edit distances spell checker, while my solution iterates over all possible edit distances and checks if these words are in the train data, which makes my model more accurately predict the correction."]}],"metadata":{"colab":{"provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"datasetId":4554327,"sourceId":7782374,"sourceType":"datasetVersion"}],"dockerImageVersionId":30664,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
