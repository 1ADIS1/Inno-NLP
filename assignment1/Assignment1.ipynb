{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKboZnAdgrRM"
      },
      "source": [
        "# [NLP] Assignment 1: Tokenization\n",
        "\n",
        "In this assignment, you need to tokenize the text of the Twitter(X) users posts(tweets). The assignment consists of two tasks. When you finish all the tasks, create a GitHub repository for this assignment (you can use this repo later for the other assignments) and submit this notebook in the repository. Leave `requirements.txt` file if your code requires additional installations. Submit the link to the repository in Moodle.\n",
        "\n",
        "The [data](https://drive.google.com/file/d/15x_wPAflvYQ2Xh38iNQGrqUIWLj5l5Nw/view?usp=share_link) contains 5 files whereby each contains 44 tweets. Each tweet is separated by a newline."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLDjjAvemUP_"
      },
      "source": [
        "## Task 1. Tokenize some tweets manually (20 points)\n",
        "\n",
        "As a first task you need to tokenize first 15 tweets from `file2` by hand. This will allow you to understand the problem from a linguistic point of view. The guidelines for tweet tokenization are as follows:\n",
        "\n",
        "- Each smiley is a separate token\n",
        "- Each hashtag is an individual token. Each user reference is an individual token\n",
        "- If a word has spaces between them then it is converted to a single token\n",
        "- All punctuations are individual tokens. This includes double-quotes and single quotes also\n",
        "- A URL is a single token\n",
        "\n",
        "Example of output\n",
        "\n",
        "    Input tweet\n",
        "    @xfranman Old age has made N A T O!\n",
        "\n",
        "    Tokenized tweet (separated by comma)\n",
        "    @xfranman , Old , age , has , made , NATO , !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KKKwTidnzUw"
      },
      "source": [
        "\n",
        "    1. Input tweet\n",
        "    Camping in Maine for the weekend. Hey Dad, Mama Loves YOU: http://www.mamapalooza.com\n",
        "    \n",
        "    1. Tokenized tweet\n",
        "    Camping , in , Maine , for , the , weekend , . , Hey , Dad , , , Mama , Loves , YOU , : ,http://www.mamapalooza.com\n",
        "\n",
        "    2. Input tweet\n",
        "    Its american tradition bitch\n",
        "    \n",
        "    2. Tokenized tweet\n",
        "    Its , american, tradition , bitch\n",
        "\n",
        "    3. Input tweet\n",
        "    @ThroughTheVoid They love it! The only pleasure they get in life. I actually do that. I'm sure I hear a tiny squeak... Then louder ones\n",
        "\n",
        "    3. Tokenized tweet\n",
        "    @ThroughTheVoid , They , love , it , ! , The , only , pleasure , they , get , in , life , . , I , actually , do , that , . , I'm , sure , I , hear , a , tiny , squeak , ... , Then , louder , ones\n",
        "\n",
        "    4. Input tweet\n",
        "    \" RT @latti: @AbsoHilare stop tweeting in church! Lol <--- \"\"I tweet because I'm happy, I tweet because I'm free\"\" LOL!\"\n",
        "\n",
        "    4. Tokenized tweet\n",
        "    \" , RT , @latti, : , @AbsoHilare , stop , tweeting , in , church , ! , Lol , <--- , \"\" , I , tweet , because , I'm , happy , , , I , tweet , because , I'm , free, \"\" , LOL , !, \",\n",
        "\n",
        "    5. Input tweet\n",
        "    Samsung Mini S2 portable HDD graced with colors that perfectly match your tacky beach gear: Sammy's done it aga.. http://tinyurl.com/lb5p6m\n",
        "\n",
        "    5. Tokenized tweet\n",
        "    Samsung , Mini , S2 , portable , HDD , graced , with , colors , that , perfectly , match , your , tacky , beach , gear, : , Sammy's , done , it , aga, .. , http://tinyurl.com/lb5p6m\n",
        "\n",
        "    6. Input tweet\n",
        "    @dialloc congrats on finding your way over. it may be slow going at first. hang in there. it's kinda cool when u get up to speed.\n",
        "\n",
        "    6. Tokenized tweet\n",
        "    @dialloc , congrats , on , finding , your , way , over , . , it , may , be , slow , going , at , first , . , hang , in , there , . , it's , kinda , cool , when , u , get , up , to , speed , .\n",
        "\n",
        "    7. Input tweet\n",
        "    iPhone activation delays continue, Apple offers $30 http://twt.gs/l3Ki\n",
        "\n",
        "    7. Tokenized tweet\n",
        "    iPhone , activation , delays , continue , , , Apple , offers , $30 , http://twt.gs/l3Ki\n",
        "\n",
        "    8. Input tweet\n",
        "    RT @GoogleAtWork Gmail maximum attachment size now 25MB http://bit.ly/62mjw Nice!!!\n",
        "\n",
        "    8. Tokenized tweet\n",
        "    RT , @GoogleAtWork , Gmail , maximum , attachment , size , now , 25MB , http://bit.ly/62mjw , Nice , !!!\n",
        "\n",
        "    9. Input tweet\n",
        "    RT @acfou The Ads Won Awards for Crispin; But Did Nothing for Client BurgerKing's Sales/Marketshare - Big Surprise - http://ping.fm/vw8TI\n",
        "\n",
        "    9. Tokenized tweet\n",
        "    RT , @acfou , The , Ads , Won , Awards , for , Crispin , ; , But , Did , Nothing , for , Client , BurgerKing's , Sales/Marketshare , - , Big , Surprise , - , http://ping.fm/vw8TI\n",
        "\n",
        "    10. Input tweet\n",
        "    Hey doll! Great I missed True Blood yday boo lol Rt @FrankBanuat78 @jhillstephens Hello Sunshine how are u today? :-)\n",
        "\n",
        "    10. Tokenized tweet\n",
        "    Hey , doll , ! , Great , I , missed , True , Blood , yday , boo , lol , Rt , @FrankBanuat78 , @jhillstephens , Hello , Sunshine , how , are , u , today , ? , :-)\n",
        "\n",
        "    11. Input tweet\n",
        "    Australian artist Pogo made these free songs primarily from sampled audio from Alice In Wonderland. http://www.last.fm/music/Pogo/Wonderland\n",
        "\n",
        "    11. Tokenized tweet\n",
        "    Australian , artist , Pogo , made , these , free , songs , primarily , from , sampled , audio , from , Alice , In , Wonderland , . , http://www.last.fm/music/Pogo/Wonderland\n",
        "\n",
        "    12. Input tweet\n",
        "    @mppritchard they wanted to sell all the preorders & then sell all of the ones they had in stock to those that just walked in. Can't do both\n",
        "\n",
        "    12. Tokenized tweet\n",
        "    @mppritchard , they , wanted , to , sell , all , the , preorders , & , then , sell , all , of , the , ones , they , had , in , stock , to , those , that , just , walked , in , . , Can't , do , both\n",
        "\n",
        "    13. Input tweet\n",
        "    Incoming: Frightened Rabbit, Sept. 22 (Tucson): If Fat Cat Records is going to send three great bands from Scot.. http://tinyurl.com/nz6xcv\n",
        "\n",
        "    13. Tokenized tweet\n",
        "    Incoming , : , Frightened , Rabbit , , , Sept. , 22 , ( , Tucson , ): , If , Fat , Cat , Records , is , going , to , send , three , great , bands , from , Scot.. , \n",
        "    http://tinyurl.com/nz6xcv\n",
        "\n",
        "    14. Input tweet\n",
        "    Hey @ginoandfran please greet philip! (GinoandFran live > http://ustre.am/2YyQ)\n",
        "\n",
        "    14. Tokenized tweet\n",
        "    Hey , @ginoandfran , please , greet , philip , ! ,( , GinoandFran , live , > , http://ustre.am/2YyQ , )\n",
        "\n",
        "    15. Input tweet\n",
        "    Ik weet niet wie er achter de T-Mobile iPhone Twitter zit maar ik vind het niet echt 'corporate' taalgebruik... Best vreemd eigenlijk\n",
        "\n",
        "    15. Tokenized tweet\n",
        "    Ik , weet , niet , wie , er , achter , de , T-Mobile , iPhone , Twitter , zit , maar , ik , vind , het , niet , echt , 'corporate' , taalgebruik , ... , Best , vreemd , eigenlijk\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2J2AD2nmUhi"
      },
      "source": [
        "## Task 2. Implement [Byte-Pair Encoding(BPE)](https://arxiv.org/pdf/1508.07909.pdf) Tokenizer (80 points)\n",
        "\n",
        "### Task 2.1. Implementation (60 points)\n",
        "\n",
        "Implement the tokenizer as the BPETokenizer class:\n",
        "* Implement `train` method that learns merges and builds the vocabulary of the specified `vocab_size` (25 points).\n",
        "* Implement `tokenize` method that should tokenize the text according to the learnt merges (25 points).\n",
        "\n",
        "Your code should have docstrings and comments (10 points)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Material that I've referenced: https://huggingface.co/learn/nlp-course/chapter6/5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import List\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "class BPETokenizer:\n",
        "    \"\"\"\n",
        "    TODO\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size: int) -> None:\n",
        "        \"\"\"\n",
        "        Initialise corpus vocabulary from the given size.\n",
        "        \"\"\"\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.vocab = set()\n",
        "        self.merges = {}\n",
        "\n",
        "        # Frequency pairs are sets of split words with number of appearences in corpus,\n",
        "        # which are used for merging words in vocabulary.\n",
        "        # I use here defaultdict because it returns default value of 0 for every new key.\n",
        "        self.frequency_words = defaultdict(int)\n",
        "\n",
        "    def compute_pair_freqs(self, splits: List):\n",
        "        \"\"\"\n",
        "        TODO\n",
        "        \"\"\"\n",
        "\n",
        "        pair_freqs = defaultdict(int)\n",
        "        for word, freq in self.frequency_words.items():\n",
        "            split = splits[word]\n",
        "            if len(split) == 1:\n",
        "                continue\n",
        "            for i in range(len(split) - 1):\n",
        "                pair = (split[i], split[i + 1])\n",
        "                pair_freqs[pair] += freq\n",
        "        return pair_freqs\n",
        "\n",
        "\n",
        "    def train(self, corpus: List[str]) -> None:\n",
        "        \"\"\"\n",
        "        Builds an initial vocabulary and adds learned merges to it until the vocab_size is reached.\n",
        "        If the algorithm cannot reach the given vocabulary size, it will stop.\n",
        "        \"\"\"\n",
        "\n",
        "        for word in corpus:\n",
        "            self.frequency_words[word] += 1\n",
        "\n",
        "        print(\"Frequency words: \", self.frequency_words)\n",
        "\n",
        "        # Comprise the base vocabulary and split the words into chars\n",
        "        splits = {}\n",
        "        for unique_word in self.frequency_words.keys():\n",
        "            splits[unique_word] = [c for c in unique_word]\n",
        "            for letter in unique_word:\n",
        "                self.vocab.add(letter)\n",
        "        \n",
        "        print(\"Base vocab: \", self.vocab)\n",
        "        print(\"Splits: \", splits)\n",
        "\n",
        "        # Merges loop\n",
        "        # iteration = 0\n",
        "        # last_vocab_size = 0\n",
        "        # while len(self.vocab) < self.vocab_size:\n",
        "        #     print(\"Merge iteration:\", iteration)\n",
        "        #     iteration += 1\n",
        "\n",
        "        #     if last_vocab_size == len(self.vocab):\n",
        "        #         print(\"Algorithm couldn't achieve given vocabulary size. Try to make it smaller.\")\n",
        "        #         break\n",
        "\n",
        "        #     last_vocab_size = len(self.vocab)\n",
        "\n",
        "        #     # Put some random default pair.\n",
        "        #     most_frequent_pair = [('a', 'b'), 0]\n",
        "        #     for word_to_count, chars_to_count in frequency_words.items():\n",
        "\n",
        "        #         # Pair to count\n",
        "        #         for i in range(len(chars_to_count) - 2):\n",
        "        #             pair_to_count = (chars_to_count[i], chars_to_count[i + 1])\n",
        "        #             count = chars_to_count[-1]\n",
        "        #             # print(\"Pair to check: \", pair_to_count)\n",
        "\n",
        "        #             # All other pairs\n",
        "        #             for word_to_compare, chars_to_compare in frequency_words.items():\n",
        "\n",
        "        #                 # Skip comparison with the same words\n",
        "        #                 if word_to_count == word_to_compare:\n",
        "        #                     continue\n",
        "\n",
        "        #                 for j in range(len(chars_to_compare) - 2):\n",
        "        #                     pair_to_compare = (chars_to_compare[j], chars_to_compare[j + 1])\n",
        "\n",
        "        #                     if pair_to_count == pair_to_compare:\n",
        "        #                         count += chars_to_compare[-1]\n",
        "                    \n",
        "        #             if count > most_frequent_pair[1]:\n",
        "        #                 most_frequent_pair = [pair_to_count, count]\n",
        "                    \n",
        "        #     # print(\"Most freq pair:\", most_frequent_pair)\n",
        "\n",
        "        #     # Add most frequent pair to vocabulary\n",
        "        #     most_freq_word = most_frequent_pair[0][0] + most_frequent_pair[0][1]\n",
        "        #     self.vocab[most_freq_word] = None\n",
        "\n",
        "        #     # Add that pair to the merges dictionary\n",
        "        #     self.merges[(most_frequent_pair[0][0], most_frequent_pair[0][1])] = most_freq_word\n",
        "\n",
        "        #     # Merge most frequent pair in the corpus\n",
        "        #     for word, chars in frequency_words.items():\n",
        "        #         for i in range(len(chars) - 2):\n",
        "        #             if str(chars[i]).isdigit() or str(chars[i + 1]).isdigit():\n",
        "        #                 continue\n",
        "        #             another_pair = (chars[i], chars[i + 1])\n",
        "        #             if most_frequent_pair[0] == another_pair:\n",
        "        #                 # Merge\n",
        "        #                 chars[i] = most_freq_word\n",
        "        #                 del chars[i + 1]\n",
        "            \n",
        "        #     # print(\"Merged corpus: \", frequency_words)\n",
        "\n",
        "    def tokenize(self, text: str) -> List[str]:\n",
        "        \"\"\"\n",
        "        Tokenizes given text with learned merges from train function.\n",
        "        \"\"\"\n",
        "\n",
        "        tokenization_result = text.split()\n",
        "\n",
        "        splits = [[l for l in word] for word in tokenization_result]\n",
        "        for pair, merge in self.merges.items():\n",
        "            for idx, split in enumerate(splits):\n",
        "                i = 0\n",
        "                while i < len(split) - 1:\n",
        "                    if split[i] == pair[0] and split[i + 1] == pair[1]:\n",
        "                        split = split[:i] + [merge] + split[i + 2 :]\n",
        "                    else:\n",
        "                        i += 1\n",
        "                splits[idx] = split\n",
        "\n",
        "        return sum(splits, [])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['hug', 'hug', 'hug', 'hug', 'hug', 'hug', 'hug', 'hug', 'hug', 'hug', 'pug', 'pug', 'pug', 'pug', 'pug', 'pun', 'pun', 'pun', 'pun', 'pun', 'pun', 'pun', 'pun', 'pun', 'pun', 'pun', 'pun', 'bun', 'bun', 'bun', 'bun', 'hugs', 'hugs', 'hugs', 'hugs', 'hugs']\n"
          ]
        }
      ],
      "source": [
        "test_corpus = [\"hug\"] * 10\n",
        "test_corpus.extend([\"pug\"] * 5)\n",
        "test_corpus.extend([\"pun\"] * 12)\n",
        "test_corpus.extend([\"bun\"] * 4)\n",
        "test_corpus.extend([\"hugs\"] * 5)\n",
        "\n",
        "print(test_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Frequency words:  defaultdict(<class 'int'>, {'hug': 10, 'pug': 5, 'pun': 12, 'bun': 4, 'hugs': 5})\n",
            "Base vocab:  {'p', 'u', 'n', 'h', 'b', 'g', 's'}\n",
            "Splits:  {'hug': ['h', 'u', 'g'], 'pug': ['p', 'u', 'g'], 'pun': ['p', 'u', 'n'], 'bun': ['b', 'u', 'n'], 'hugs': ['h', 'u', 'g', 's']}\n"
          ]
        }
      ],
      "source": [
        "test_tokenizer = BPETokenizer(vocab_size=10)\n",
        "test_tokenizer.train(test_corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['hug', 'y', 'p', 'ug', 'g', 'y', 'm', 'ug', 'g', 'y', 'm', 'o', 'o', '.']"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_tokenizer.tokenize(\"hugy puggy muggy moo.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 2.2. Analysis on Tweets Dataset (10 points)\n",
        "\n",
        "Train the BPE tokenizer on the tweets dataset. Try to tokenize the tweets with the tokenizer of different `vocab_size`. For example, train the BPE tokenizer with `vocab_size` of [base_vocab_size, 250, 500, 750, 1000]. Plot the dependency of the average length of the tokenized tweet by `vocab_size` to analyze how `vocab_size` affects the length of the tokenized tweet on average. Tell what `vocab_size` is preferrable and why."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Merge iteration: 0\n",
            "Merge iteration: 1\n",
            "Merge iteration: 2\n",
            "Merge iteration: 3\n",
            "Merge iteration: 4\n",
            "Merge iteration: 5\n",
            "Merge iteration: 6\n",
            "Merge iteration: 7\n",
            "Merge iteration: 8\n",
            "Merge iteration: 9\n",
            "Merge iteration: 10\n",
            "Merge iteration: 11\n",
            "Merge iteration: 12\n",
            "Merge iteration: 13\n",
            "Merge iteration: 14\n",
            "Merge iteration: 15\n",
            "Merge iteration: 16\n",
            "Merge iteration: 17\n",
            "Merge iteration: 18\n",
            "Merge iteration: 19\n",
            "Merge iteration: 20\n",
            "Merge iteration: 21\n",
            "Merge iteration: 22\n",
            "Merge iteration: 23\n",
            "Merge iteration: 24\n",
            "Unexpected exception formatting exception. Falling back to standard exception\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"d:\\Programs\\Python_3_11\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3460, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"C:\\Users\\alifa\\AppData\\Local\\Temp\\ipykernel_1700\\2226121284.py\", line 8, in <module>\n",
            "    tokenizer.train(full_text.split())\n",
            "  File \"C:\\Users\\alifa\\AppData\\Local\\Temp\\ipykernel_1700\\1076302023.py\", line -1, in train\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"d:\\Programs\\Python_3_11\\Lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2057, in showtraceback\n",
            "    stb = self.InteractiveTB.structured_traceback(\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\Programs\\Python_3_11\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1288, in structured_traceback\n",
            "    return FormattedTB.structured_traceback(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\Programs\\Python_3_11\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1177, in structured_traceback\n",
            "    return VerboseTB.structured_traceback(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\Programs\\Python_3_11\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 1030, in structured_traceback\n",
            "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\Programs\\Python_3_11\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 960, in format_exception_as_a_whole\n",
            "    frames.append(self.format_record(record))\n",
            "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\Programs\\Python_3_11\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 870, in format_record\n",
            "    frame_info.lines, Colors, self.has_colors, lvals\n",
            "    ^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\Programs\\Python_3_11\\Lib\\site-packages\\IPython\\core\\ultratb.py\", line 704, in lines\n",
            "    return self._sd.lines\n",
            "           ^^^^^^^^^^^^^^\n",
            "  File \"d:\\Programs\\Python_3_11\\Lib\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
            "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
            "                                               ^^^^^^^^^^^^^^\n",
            "  File \"d:\\Programs\\Python_3_11\\Lib\\site-packages\\stack_data\\core.py\", line 734, in lines\n",
            "    pieces = self.included_pieces\n",
            "             ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\Programs\\Python_3_11\\Lib\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
            "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
            "                                               ^^^^^^^^^^^^^^\n",
            "  File \"d:\\Programs\\Python_3_11\\Lib\\site-packages\\stack_data\\core.py\", line 681, in included_pieces\n",
            "    pos = scope_pieces.index(self.executing_piece)\n",
            "                             ^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"d:\\Programs\\Python_3_11\\Lib\\site-packages\\stack_data\\utils.py\", line 144, in cached_property_wrapper\n",
            "    value = obj.__dict__[self.func.__name__] = self.func(obj)\n",
            "                                               ^^^^^^^^^^^^^^\n",
            "  File \"d:\\Programs\\Python_3_11\\Lib\\site-packages\\stack_data\\core.py\", line 660, in executing_piece\n",
            "    return only(\n",
            "           ^^^^^\n",
            "  File \"d:\\Programs\\Python_3_11\\Lib\\site-packages\\executing\\executing.py\", line 190, in only\n",
            "    raise NotOneValueFound('Expected one value, found 0')\n",
            "executing.executing.NotOneValueFound: Expected one value, found 0\n"
          ]
        }
      ],
      "source": [
        "tokenizer = BPETokenizer(vocab_size=250)\n",
        "\n",
        "full_text = \"\"\n",
        "for i in range(1, 6):\n",
        "    with open(f\"../assignment1/data/file{i}\", \"r\") as f:\n",
        "        full_text += f.read()\n",
        "\n",
        "tokenizer.train(full_text.split())\n",
        "tokenizer.tokenize(full_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Task 2.3. Analysis on Dataset of Different Language (10 points)\n",
        "\n",
        "Find a small dataset of texts in a language other than English. The dataset size should be not greater than several megabytes.\n",
        "\n",
        "Train the BPE tokenizer on the dataset that you found. Try to tokenize the sentences from this dataset with the tokenizer of different `vocab_size`. Plot the dependency of the average length of the tokenized sentence by `vocab_size` to analyze how `vocab_size` affects the length of the tokenized sentence on average.\n",
        "\n",
        "Tell how how the average length of the tokenized sentence differs from the average length of the tokenized tweet. Explain why. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Your code here"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
